{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MATGI6xNJda"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haA0hypBRuOE"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3COzQBSiWoXo"
   },
   "source": [
    "## Environment Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBi_5RsWNpEb"
   },
   "outputs": [],
   "source": [
    "def is_colab(): # Is environment a Google Colab instance?\n",
    "    return 'google.colab' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oahLiUhNsCs"
   },
   "outputs": [],
   "source": [
    "def is_kaggle(): # Is environment a Kaggle Notebook?\n",
    "    return os.environ.get('PWD') == '/kaggle/working' and os.environ.get('KAGGLE_URL_BASE') == 'https://www.kaggle.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLwPF6NvNvNx"
   },
   "outputs": [],
   "source": [
    "print(os.getcwd(), \"----\", os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEPDgKEBRC2z"
   },
   "source": [
    "## Config\n",
    "\n",
    "Pre-Set the Dataset Folder\n",
    "\n",
    "Automatic Dataset Download and Drive mounting for Colab\n",
    "\n",
    "Kaggle needs an Dataset createt like the folder Structure and needs to be integrated in the Notebook\n",
    "\n",
    "Local needs an manual Downloaded Dataset \n",
    "\n",
    "Kaggle: \n",
    "```\n",
    "/semeval2024-task2/semeval_task2_training_data\n",
    "│   dev.json\n",
    "│   train.json   \n",
    "└───CT json\n",
    "    └───...\n",
    "```\n",
    "\n",
    "\n",
    "Colab:\n",
    "```\n",
    "MyDrive\n",
    "│   dev.json\n",
    "│   train.json   \n",
    "└───CT json\n",
    "    └───...\n",
    "```\n",
    "\n",
    "\n",
    "Local:\n",
    "```\n",
    "./tmp/Task-2-SemEval-2024/training_data\n",
    "│   dev.json\n",
    "│   train.json   \n",
    "└───CT json\n",
    "    └───...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Folder and Dataset Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLzOVEh-REAE"
   },
   "outputs": [],
   "source": [
    "FOLDER = \"./dataset/training_data\" #Local\n",
    "if is_colab():  FOLDER = \"/content/drive/MyDrive\" #Colab\n",
    "if is_kaggle(): FOLDER = \"/kaggle/input/semeval2024-task2/semeval_task2_training_data\" #Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrUDXqEFRJdJ"
   },
   "outputs": [],
   "source": [
    "FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2ASaCP_RFoD"
   },
   "outputs": [],
   "source": [
    "LOG_FOLDER = \"./\" #Local\n",
    "if is_colab():  LOG_FOLDER = \"/content/drive/MyDrive\" #Colab\n",
    "if is_kaggle(): LOG_FOLDER = \"/kaggle/working\" #Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJQfhCtDRL9f"
   },
   "outputs": [],
   "source": [
    "LOG_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4590QaN8N0__",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ixj6FZwMNzHs"
   },
   "outputs": [],
   "source": [
    "if is_colab():\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Rtim2IlGNdW"
   },
   "outputs": [],
   "source": [
    "if is_kaggle():\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRYGJ3uzPQEt",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Install Dataset for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMkpsHt7PTRu"
   },
   "outputs": [],
   "source": [
    "if is_colab():\n",
    "  !ls\n",
    "  %cd /content/drive/MyDrive\n",
    "\n",
    "  PROJECT_DIR = '/content/drive/MyDrive/semeval-2024'\n",
    "  PROJECT_GITHUB_URL = 'https://github.com/ai-systems/Task-2-SemEval-2024.git'\n",
    "\n",
    "  if not os.path.isdir(PROJECT_DIR):\n",
    "    !git clone {PROJECT_GITHUB_URL}\n",
    "  else:\n",
    "    %cd {PROJECT_DIR}\n",
    "    !git pull {PROJECT_GITHUB_URL}\n",
    "\n",
    "  !unzip -n /content/drive/MyDrive/Task-2-SemEval-2024/training_data.zip\n",
    "\n",
    "  %cd /content\n",
    "  !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_colab() and not is_kaggle():\n",
    "    !cd dataset\n",
    "    shutil.unpack_archive(\"dataset/training_data.zip\", \"dataset\") \n",
    "    !cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hufo5YTOdhNB",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports\n",
    "\n",
    "Installing and importing all the neccesary python libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EikkdxYdSCK"
   },
   "outputs": [],
   "source": [
    "if is_colab():\n",
    "  !pip install datasets\n",
    "  !pip install pytorch_lightning\n",
    "  !pip install torchmetrics\n",
    "  !pip install transformers\n",
    "  #!pip install torchinfo\n",
    "  !pip install sentencepiece\n",
    "  !pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNMERObDn2WH"
   },
   "outputs": [],
   "source": [
    "if not is_colab() and not is_kaggle():\n",
    "  !pip install json\n",
    "  !pip install numpy\n",
    "  !pip install matplotlib\n",
    "  #!pip install seaborn\n",
    "  #!pip install tqdm\n",
    "  #!pip install pandas\n",
    "  !pip install torch\n",
    "  !pip install pytorch_lightning\n",
    "  !pip install torchmetrics\n",
    "  !pip install transformers\n",
    "  !pip install sentencepiece\n",
    "  !pip install datasets\n",
    "  !pip install mlxtend\n",
    "  !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IabRAu_-dK-W"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any\n",
    "import os\n",
    "import sys\n",
    "#import timeit\n",
    "import random\n",
    "from collections import Counter\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sn\n",
    "#from tqdm import tqdm\n",
    "#import pandas as pd\n",
    "\n",
    "import torch\n",
    "#import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "#from pytorch_lightning.utilities import CombinedLoader\n",
    "import torchmetrics\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functools import lru_cache\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "\n",
    "#from pytorch_lightning.utilities.model_summary import summarize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBXmlO8tdnpa",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2l8C3UInddba"
   },
   "outputs": [],
   "source": [
    "#@lru_cache(maxsize=1024)\n",
    "def get_file(pth:str) -> dict:\n",
    "    \"\"\"Loading a JSON File\n",
    "\n",
    "    Args:\n",
    "        pth (str): Path to File\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with the Entries of the json file\n",
    "    \"\"\"\n",
    "\n",
    "    with open(pth) as json_file:\n",
    "        dev = json.load(json_file)\n",
    "    return dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6cCivOF0A6F"
   },
   "outputs": [],
   "source": [
    "def extract(itm:dict, permutate:bool=False) -> Tuple[List[str], str]:\n",
    "    \"\"\"Return all the neccessary information of one Datapoint which includes the relevant Statement, the Section and\n",
    "    Primary Trial. If the type is Comparison also the Secondary Trial is added\n",
    "\n",
    "    Args:\n",
    "        itm (dict): Datapoint/Hypothesis\n",
    "        permutate (bool, optional): Activates Permutation of Primary and Secondary Trial. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], str]: Returns List[Statement, Section, Primary, Secondary] and Label of a Datapoint\n",
    "    \"\"\"\n",
    "\n",
    "    primary   = get_file(f\"{FOLDER}/CT json/{itm['Primary_id']}.json\"  )[itm['Section_id']]\n",
    "    secondary = get_file(f\"{FOLDER}/CT json/{itm['Secondary_id']}.json\")[itm['Section_id']] if itm['Type'] == 'Comparison' else []\n",
    "    if permutate: random.shuffle(primary)\n",
    "    if permutate: random.shuffle(secondary)\n",
    "    return [itm['Statement'], itm['Section_id'], \", \".join([i.strip() for i in primary]), \", \".join([i.strip() for i in secondary])], itm['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_RwydtAnvmv"
   },
   "outputs": [],
   "source": [
    "def split_v2(strng:str, sep:str, pos:int) -> List[str]:\n",
    "    \"\"\"splitting a input string with a seperator and putting the elements before a position and after a position with the same seperator together\n",
    "\n",
    "    Args:\n",
    "        strng (str): Input String\n",
    "        sep (str): Seperator\n",
    "        pos (int): position which String are being fused togehter\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Seperated String\n",
    "    \n",
    "\n",
    "    Example Input 1: \n",
    "        strng='aaa,bbb,ccc,ddd'\n",
    "        sep  =','\n",
    "        pos  =3\n",
    "        --------------> ['aaa,bbb,ccc', 'ddd']\n",
    "    Example Input 2: \n",
    "        strng='aaa,bbb,ccc,ddd'\n",
    "        sep  =','\n",
    "        pos  =2\n",
    "        --------------> ['aaa,bbb','ccc,ddd']\n",
    "    \"\"\"\n",
    "    strng = strng.split(sep)\n",
    "    return [sep.join(strng[:pos]), sep.join(strng[pos:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15GIW-qTn3cE"
   },
   "outputs": [],
   "source": [
    "def class_to_cos(inp:torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Mapping 0 Class to -1\n",
    "    Mapping 1 Class to +1\n",
    "    \n",
    "    Args:\n",
    "        inp (torch.Tensor): Input Tensor\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Mapped Tensor\n",
    "    \"\"\"\n",
    "    inp = torch.clone(inp.detach())\n",
    "    inp[inp == 0] = -1\n",
    "    inp[inp == 1] = 1\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHppNU2XnzFo"
   },
   "outputs": [],
   "source": [
    "def cos_to_class(inp:torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Mapping < 0 to Class 0 and \n",
    "    Mapping > 0 to Class 1\n",
    "\n",
    "    Args:\n",
    "        inp (torch.Tensor): Input Tensor\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Mapped Tensor\n",
    "    \"\"\"\n",
    "    inp = torch.clone(inp.detach())\n",
    "    inp[inp < 0] = 0\n",
    "    inp[inp > 0] = 1\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTpiUWNYkArn"
   },
   "outputs": [],
   "source": [
    "def get_nonlin_func(nonlin: str):\n",
    "    \"\"\"Returns a NON-Linearity Activation function\n",
    "\n",
    "    Args:\n",
    "        nonlin (str): Function name\n",
    "\n",
    "    Raises:\n",
    "        ValueError: no suitable functionn choosen\n",
    "\n",
    "    Returns:\n",
    "        func: Activation Function\n",
    "    \"\"\"\n",
    "    if nonlin == \"tanh\":    return torch.tanh\n",
    "    if nonlin == \"relu\":    return torch.relu\n",
    "    if nonlin == \"gelu\":    return torch.nn.functional.gelu\n",
    "    if nonlin == \"sigmoid\": return torch.sigmoid\n",
    "    raise ValueError(\"Unsupported nonlinearity!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upd(dic:dict, key:Any, val:Any) -> dict:\n",
    "    \"\"\"Updates A dictionary key-value Entry\n",
    "\n",
    "    Args:\n",
    "        dic (dict): Input Dictionary\n",
    "        key (Any): Key which Value needs to be updated\n",
    "        val (Any): ned Value\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated Dictionary\n",
    "    \"\"\"\n",
    "    dic.update({key: val})\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDgFmkoud1fm"
   },
   "source": [
    "## Dataset & Dataloader & LightningDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epvStwjYNOek"
   },
   "outputs": [],
   "source": [
    "label_enum = {\"Contradiction\":0,\"Entailment\":1} #Text Classes to Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLdCnFdMSWZE"
   },
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3MtRIqXdtxb"
   },
   "outputs": [],
   "source": [
    "class SemEvalDatasetModule:\n",
    "    def __init__(self, dataset, permutate=False):\n",
    "        self.dataset = dataset\n",
    "        self.permutate = permutate\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the length of the Dataset\n",
    "\n",
    "        Returns:\n",
    "            int: Length of the Dataset\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def name(self) -> str:\n",
    "        \"\"\"Returns a Name\n",
    "\n",
    "        Returns:\n",
    "            str: name of the Class\n",
    "        \"\"\"\n",
    "        return 'semeval'\n",
    "\n",
    "    def __getitem__(self, idx:int)-> dict:\n",
    "        \"\"\"Returns the Datapoint from the Dataset on position index\n",
    "\n",
    "        Args:\n",
    "            idx (int): position in the set\n",
    "\n",
    "        Returns:\n",
    "            dict: Sencence and Label\n",
    "        \"\"\"\n",
    "        tmp = extract(self.dataset[idx], self.permutate)\n",
    "        return {'image': \" [SEP] \".join(tmp[0]), 'label':torch.tensor(label_enum[tmp[1]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tog-HwxfIxr_"
   },
   "outputs": [],
   "source": [
    "class ScifactDatasetModule:\n",
    "    def __init__(self):\n",
    "        \"\"\"Scifact Dataset Loader loading without the Neutral Class with a mapping to the text Classes of SemEval\n",
    "        Due to small dataset train and validation is fused an be interpreted as one\n",
    "        Original Classes being either SUPPORT, NEI, CONTRADICT\n",
    "        \n",
    "        Task is to check the entailment if the claim matches to a paper title and abstract\n",
    "        \n",
    "        https://huggingface.co/datasets/allenai/scifact_entailment\n",
    "        \"\"\"\n",
    "        self.ds = load_dataset(\"allenai/scifact_entailment\")\n",
    "        self.dataset = [upd(i, \"label\", \"Entailment\" if (i['verdict'] == \"SUPPORT\") else \"Contradiction\") for i in [*self.ds[\"train\"], *self.ds[\"validation\"]] if not (i['verdict'] == \"NEI\")]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the length of the Dataset\n",
    "\n",
    "        Returns:\n",
    "            int: Length of the Dataset\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def name(self) -> str:\n",
    "        \"\"\"Returns a Name\n",
    "\n",
    "        Returns:\n",
    "            str: name of the Class\n",
    "        \"\"\"\n",
    "        return 'scifact'\n",
    "\n",
    "    def __getitem__(self, idx:int)-> dict:\n",
    "        \"\"\"Returns the Datapoint from the Dataset on position index\n",
    "\n",
    "        Args:\n",
    "            idx (int): position in the set\n",
    "\n",
    "        Returns:\n",
    "            dict: Sencence and Label\n",
    "        \"\"\"\n",
    "        return {'image': \"{} [SEP] {} [SEP] {}\".format(self.dataset[idx][\"claim\"], self.dataset[idx][\"title\"], \" \".join(self.dataset[idx][\"abstract\"])), 'label':torch.tensor(label_enum[self.dataset[idx][\"label\"]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tz_BndOrTuJg"
   },
   "outputs": [],
   "source": [
    "class HealthverDatasetModule:\n",
    "    def __init__(self):\n",
    "        \"\"\"Healthver Dataset Loader loading without the Neutral Class with a mapping to the text Classes of SemEval\n",
    "        Due to small dataset train and validation is fused an be interpreted as one\n",
    "        Original Classes being either SUPPORT, NEI, CONTRADICT\n",
    "        \n",
    "        Task is to check the entailment if the claim matches to a paper title and abstract\n",
    "        \n",
    "        https://huggingface.co/datasets/dwadden/healthver_entailment\n",
    "        \"\"\"\n",
    "        self.ds = load_dataset(\"dwadden/healthver_entailment\")\n",
    "        self.dataset = [upd(i, \"label\", \"Entailment\" if (i['verdict'] == \"SUPPORT\") else \"Contradiction\") for i in [*self.ds[\"train\"], *self.ds[\"validation\"]] if not (i['verdict'] == \"NEI\")]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the length of the Dataset\n",
    "\n",
    "        Returns:\n",
    "            int: Length of the Dataset\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def name(self) -> str:\n",
    "        \"\"\"Returns a Name\n",
    "\n",
    "        Returns:\n",
    "            str: name of the Class\n",
    "        \"\"\"\n",
    "        return 'healthver'\n",
    "\n",
    "    def __getitem__(self, idx:int)-> dict:\n",
    "        \"\"\"Returns the Datapoint from the Dataset on position index\n",
    "\n",
    "        Args:\n",
    "            idx (int): position in the set\n",
    "\n",
    "        Returns:\n",
    "            dict: Sencence and Label\n",
    "        \"\"\"\n",
    "        return {'image': \"{} [SEP] {} [SEP] {}\".format(self.dataset[idx][\"claim\"], self.dataset[idx][\"title\"], \" \".join(self.dataset[idx][\"abstract\"])), 'label':torch.tensor(label_enum[self.dataset[idx][\"label\"]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hfebzMASqZu"
   },
   "outputs": [],
   "source": [
    "class SnliDatasetModule:\n",
    "    def __init__(self, name=\"validation\"):\n",
    "        \"\"\" SNLI Dataset Loader loading without the Neutral Class with a mapping to the text Classes of SemEval\n",
    "        \n",
    "        Task is to check the entailment if the premise matches hypothesis\n",
    "        \n",
    "        https://huggingface.co/datasets/snli\n",
    "\n",
    "        Args:\n",
    "            name (str, optional): train, validation, test of snli dataset. Defaults to \"validation\".\n",
    "        \"\"\"\n",
    "        self.ds = load_dataset(\"snli\")\n",
    "        self.dataset = [upd(i, \"label\", \"Entailment\" if (i['label'] == 0) else \"Contradiction\") for i in self.ds[name] if not (i['label'] == 1)]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the length of the Dataset\n",
    "\n",
    "        Returns:\n",
    "            int: Length of the Dataset\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def name(self) -> str:\n",
    "        \"\"\"Returns a Name\n",
    "\n",
    "        Returns:\n",
    "            str: name of the Class\n",
    "        \"\"\"\n",
    "        return 'snli'\n",
    "\n",
    "    def __getitem__(self, idx:int)-> dict:\n",
    "        \"\"\"Returns the Datapoint from the Dataset on position index\n",
    "\n",
    "        Args:\n",
    "            idx (int): position in the set\n",
    "\n",
    "        Returns:\n",
    "            dict: Sencence and Label\n",
    "        \"\"\"\n",
    "        return {'image': \"{} [SEP] {}\".format(self.dataset[idx][\"premise\"], self.dataset[idx][\"hypothesis\"]), 'label':torch.tensor(label_enum[self.dataset[idx][\"label\"]])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjAUAIyaSaNz"
   },
   "source": [
    "### Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYc03UE5d4Dq"
   },
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=4, num_workers=0, permutate=False, strategy=\"\", ds_names=[], **kwargs):\n",
    "      \"\"\"Lightning Dataset Module for Loading all possible Datasets initial\n",
    "\n",
    "      Args:\n",
    "          batch_size (int, optional): Batch size for the Dataloaders. Defaults to 4.\n",
    "          num_workers (int, optional): Number of Parallel Workers. Defaults to 0.\n",
    "          permutate (bool, optional): Activates Permutation for Training. Defaults to False.\n",
    "          strategy (str, optional): Strategy Decision oneof(\"CombinedLoader\", \"CombinedDataset\", \"Pre-Post:0:50\"). Defaults to \"\".\n",
    "          ds_names (list, optional): optional Datasets for the Strategy. Defaults to [].\n",
    "      \"\"\"\n",
    "      super().__init__()\n",
    "      self.save_hyperparameters()\n",
    "      self.prepare_data_per_node = False\n",
    "      print(self.hparams)\n",
    "\n",
    "\n",
    "      #Loading Training Dataset\n",
    "      train = get_file(f\"{FOLDER}/train.json\")\n",
    "      self.train_dataset   = SemEvalDatasetModule([train[key] for key in train], self.hparams.permutate)\n",
    "      print(\"Length of SemEval Train Dataset: \", len(self.train_dataset))\n",
    "      \n",
    "      #Expanding the Dataset\n",
    "      if self.hparams.strategy:\n",
    "        assert ds_names, \"NO Dataset Choosen\"\n",
    "        print(f\"Datasets for expanding Training with strategy {self.hparams.strategy}:\")\n",
    "        self.ds = [self.train_dataset]\n",
    "        if \"snli\" in self.hparams.ds_names: self.append_train_ds(SnliDatasetModule())\n",
    "        if \"scifact\" in self.hparams.ds_names: self.append_train_ds(ScifactDatasetModule())\n",
    "        if \"healthver\" in self.hparams.ds_names: self.append_train_ds(HealthverDatasetModule())\n",
    "        \n",
    "      #Loading Validation Dataset\n",
    "      valid = get_file(f\"{FOLDER}/dev.json\")\n",
    "      self.val_dataset     = SemEvalDatasetModule([valid[key] for key in valid])\n",
    "      print(\"Length of SemEval Valid Dataset: \", len(self.val_dataset))\n",
    "        \n",
    "      #Loading Test Dataset\n",
    "      self.test_dataset    =  None\n",
    "      \n",
    "      #Loading Validation Dataset\n",
    "      self.predict_dataset = None\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "    def append_train_ds(self, ds):\n",
    "      \"\"\"Append the Dataset to the List\n",
    "\n",
    "      Args:\n",
    "          ds (Dataset): Selected Dataset\n",
    "      \"\"\"\n",
    "      print(f\"      Length of {ds.name()} Dataset: \", len(ds))\n",
    "      self.ds.append(ds)\n",
    "\n",
    "    def symm_merger(self) -> List[dict]:\n",
    "      \"\"\"Merges equaly each Dataset \n",
    "      \n",
    "      Example for 4 Datasets: 1, 2, 3, 4,    1, 2, 3, 4,    ...\n",
    "\n",
    "      Returns:\n",
    "          List[dict]: Newly Merged Dictionaries\n",
    "      \"\"\"\n",
    "      dl = [torch.utils.data.DataLoader(i,batch_size=None,num_workers=self.hparams.num_workers,shuffle=True) for i in self.ds]\n",
    "      return [{'image': j[\"image\"], 'label': j[\"label\"]} for i in zip(*dl) for j in i]\n",
    "\n",
    "\n",
    "\n",
    "    def train_dataloader(self)-> torch.utils.data.DataLoader:\n",
    "      \"\"\"Returns the Training Dataloader with the choosen hyperparameters\n",
    "\n",
    "      Returns:\n",
    "          torch.utils.data.DataLoader: Train Dataloader\n",
    "      \"\"\"\n",
    "      #equal number elements in batch (no shuffle because it destroys the structure)\n",
    "      if \"CombinedLoader\" == self.hparams.strategy:\n",
    "        return torch.utils.data.DataLoader(self.symm_merger(),batch_size=self.hparams.batch_size,num_workers=self.hparams.num_workers,shuffle=False)\n",
    "\n",
    "      #fully randomly mixind of all Datasets\n",
    "      if \"CombinedDataset\" == self.hparams.strategy:\n",
    "        return torch.utils.data.DataLoader(torch.utils.data.ConcatDataset(self.ds), batch_size=self.hparams.batch_size,num_workers=self.hparams.num_workers,shuffle=True)\n",
    "\n",
    "      #Training in several steps (3 Steps: Pre-Post:30:50 --> Epoch ds1 0-30, Epoch ds2 30-50, Epoch semeval 50-END)\n",
    "      if \"Pre-Post\" in self.hparams.strategy:\n",
    "        _split= self.hparams.strategy.split(\":\")\n",
    "        _split[0] = \"0\"\n",
    "        _split = list(map(int, _split))\n",
    "        assert len(self.ds) == len(_split), \"Number Stages does not equal Number of Datasets\"\n",
    "        return {i:torch.utils.data.DataLoader(j,batch_size=self.hparams.batch_size,num_workers=self.hparams.num_workers,shuffle=True) for i, j in zip(_split, reversed(self.ds))}\n",
    "\n",
    "      #Failsafe\n",
    "      return torch.utils.data.DataLoader(self.train_dataset,batch_size=self.hparams.batch_size,num_workers=self.hparams.num_workers,shuffle=True) \n",
    "\n",
    "\n",
    "\n",
    "    def val_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "      \"\"\"Returns the Validation Dataloader with the choosen hyperparameters and no shuffle\n",
    "\n",
    "      Returns:\n",
    "          torch.utils.data.DataLoader: Validation Dataloader\n",
    "      \"\"\"\n",
    "      return torch.utils.data.DataLoader(self.val_dataset,batch_size=self.hparams.batch_size,num_workers=self.hparams.num_workers,shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    def test_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "      \"\"\"Returns the Test Dataloader with the choosen hyperparameters and no shuffle\n",
    "\n",
    "      Returns:\n",
    "          torch.utils.data.DataLoader: Test Dataloader\n",
    "      \"\"\"\n",
    "      return torch.utils.data.DataLoader(self.test_dataset,batch_size=self.hparams.batch_size,num_workers=self.hparams.num_workers,shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    def predict_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "      \"\"\"Returns the Predict Dataloader with the choosen hyperparameters and no shuffle\n",
    "\n",
    "      Returns:\n",
    "          torch.utils.data.DataLoader: Predict Dataloader\n",
    "      \"\"\"\n",
    "      return torch.utils.data.DataLoader(self.predict_dataset,batch_size=self.hparams.batch_size,num_workers=self.hparams.num_workers,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3NUmQwkd3Ky"
   },
   "source": [
    "## Set up the Pytorch Model and LightningModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8O8avZuSR4F"
   },
   "source": [
    "### AutoModelForSequenceClassification + CELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TL80LE2fy92M"
   },
   "outputs": [],
   "source": [
    "class ClassificationModel_V1(torch.nn.Module):\n",
    "    \"\"\"Baseline Model AutoModelForSequenceClassification with CrossEntropyLoss as loss function\n",
    "    \n",
    "    model.forward():\n",
    "    | tokenize Sentence\n",
    "    | Prediction\n",
    "    | Calculate Loss\n",
    "    | Return Loss and Prediction\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,model_name:str=\"distilbert-base-uncased\", num_labels:int=2,len_embeddings:int=512):\n",
    "        super(ClassificationModel_V1,self).__init__()\n",
    "        self.len_embeddings = len_embeddings\n",
    "        self.model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=num_labels,max_position_embeddings=self.len_embeddings,ignore_mismatched_sizes=True)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name,do_lower_case=True,sep_token='[SEP]')\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self,sentence,label,device):\n",
    "        token = self.tokenizer(sentence,add_special_tokens=True,padding='max_length',truncation=True,max_length=self.len_embeddings,return_tensors=\"pt\").to(device)\n",
    "        pred = self.model(**token,labels=label)\n",
    "        loss = self.loss_fn(pred.logits, label)\n",
    "        return torch.unsqueeze(loss, -1), torch.argmax(pred.logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iG_Q2vYhStGT"
   },
   "source": [
    "### 4xSiameseBert (weight Sharing) -> (u, v, x, y)Linear + CELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ker4HGVSYDsv"
   },
   "outputs": [],
   "source": [
    "class ClassificationModel_V2(torch.nn.Module):\n",
    "    \"\"\"Sentence BERT idea with 4 Siamese Networks, where the 4 parts of the sentence run seperately trough the model\n",
    "    \n",
    "    #https://www.sbert.net/examples/training/nli/README.html\n",
    "    #https://arxiv.org/pdf/1908.10084.pdf\n",
    "    \n",
    "    model.forward():\n",
    "    | Split sentence in 4 pieces with the [SEP] token as  Splitter\n",
    "    | 4xTokenize Sentence\n",
    "    | 4x Prediction\n",
    "    | Stack Items\n",
    "    | Fully Connected Layer\n",
    "    | Calculate Loss\n",
    "    | Return Loss and Prediction\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,model_name:str=\"distilbert-base-uncased\", num_labels:int=2,len_embeddings:int=512):\n",
    "        super(ClassificationModel_V2,self).__init__()\n",
    "        self.len_embeddings = len_embeddings\n",
    "        self.model = transformers.AutoModel.from_pretrained(model_name,max_position_embeddings=self.len_embeddings,ignore_mismatched_sizes=True)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name,do_lower_case=True,sep_token='[SEP]')\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        #Feed Forward Network\n",
    "        self.dp1 = torch.nn.Dropout(p=0.1, inplace=False)\n",
    "        self.fc1 = torch.nn.Linear(3072, num_labels, bias=True)\n",
    "\n",
    "    def forward(self,sentence,label,device):\n",
    "        prepare_s  = [itm.split(\" [SEP] \") for itm in sentence]\n",
    "        tokens     = [self.tokenizer(itm,padding='max_length',truncation=True,max_length=self.len_embeddings,return_tensors=\"pt\").to(device) for itm in prepare_s]\n",
    "        embeddings = [self.model(**itm, output_hidden_states=True, return_dict=True).pooler_output for itm in tokens]\n",
    "\n",
    "        x = torch.stack(embeddings)\n",
    "        pred = self.fc1(self.dp1(x.flatten(1)))\n",
    "        loss = self.loss_fn(pred, label)\n",
    "        return torch.unsqueeze(loss, -1), torch.argmax(pred, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7Fa6cGeS5t-"
   },
   "source": [
    "### 2xSiameseBert (weight Sharing) -> (u, v)Linear + CELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrWG_mEGzSt4"
   },
   "outputs": [],
   "source": [
    "class ClassificationModel_V3(torch.nn.Module):\n",
    "    \"\"\"Sentence BERT idea with 2 Siamese Networks, where the 2 parts of the sentence run seperately trough the model\n",
    "    statement vs section [SEP] primary [SEP] secondary \n",
    "    \n",
    "    #https://www.sbert.net/examples/training/nli/README.html\n",
    "    #https://arxiv.org/pdf/1908.10084.pdf\n",
    "    \n",
    "    model.forward():\n",
    "    | Split sentence in 4 pieces with the [SEP] token as  Splitter\n",
    "    | 2xTokenize Sentence\n",
    "    | 2x Prediction\n",
    "    | Stack Items\n",
    "    | Fully Connected Layer\n",
    "    | Calculate Loss\n",
    "    | Return Loss and Prediction\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,model_name:str=\"distilbert-base-uncased\", num_labels:int=2,len_embeddings:int=512):\n",
    "        super(ClassificationModel_V3,self).__init__()\n",
    "        self.len_embeddings = len_embeddings\n",
    "        self.model = transformers.AutoModel.from_pretrained(model_name,max_position_embeddings=self.len_embeddings,ignore_mismatched_sizes=True)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name,do_lower_case=True,sep_token='[SEP]')\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        #Feed Forward Network\n",
    "        self.dp1 = torch.nn.Dropout(p=0.1, inplace=False)\n",
    "        self.fc1 = torch.nn.Linear(1536, num_labels, bias=True)\n",
    "\n",
    "    def forward(self,sentence,label,device):\n",
    "        prepare_s  = [split_v2(itm, \" [SEP] \", 1) for itm in sentence]\n",
    "        tokens     = [self.tokenizer(itm,padding='max_length',truncation=True,max_length=self.len_embeddings,return_tensors=\"pt\").to(device) for itm in prepare_s]\n",
    "        embeddings = [self.model(**itm, output_hidden_states=True, return_dict=True).pooler_output for itm in tokens]\n",
    "\n",
    "        x = torch.stack(embeddings)\n",
    "        pred = self.fc1(self.dp1(x.flatten(1)))\n",
    "        loss = self.loss_fn(pred, label)\n",
    "        return torch.unsqueeze(loss, -1), torch.argmax(pred, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0CAoxwQS9bz"
   },
   "source": [
    "### 2xSiameseBert (weight Sharing) -> (u, v, |u-v|)Linear + CELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxxyjbYw_TLT"
   },
   "outputs": [],
   "source": [
    "class ClassificationModel_V4(torch.nn.Module):\n",
    "    \"\"\"Sentence BERT idea with 2 Siamese Networks, where the 2 parts of the sentence run seperately trough the model\n",
    "    statement vs section [SEP] primary [SEP] secondary \n",
    "    \n",
    "    #https://www.sbert.net/examples/training/nli/README.html\n",
    "    #https://arxiv.org/pdf/1908.10084.pdf\n",
    "    \n",
    "    model.forward():\n",
    "    | Split sentence in 2 pieces with the [SEP] token as  Splitter\n",
    "    | 2xTokenize Sentence\n",
    "    | 2x Prediction\n",
    "    | Stack Items\n",
    "    | Fully Connected Layer+\n",
    "    | Calculate Loss\n",
    "    | Return Loss and Prediction\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,model_name:str=\"distilbert-base-uncased\", num_labels:int=2,len_embeddings:int=512):\n",
    "        super(ClassificationModel_V4,self).__init__()\n",
    "        self.len_embeddings = len_embeddings\n",
    "        self.model = transformers.AutoModel.from_pretrained(model_name,max_position_embeddings=self.len_embeddings,ignore_mismatched_sizes=True)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name,do_lower_case=True,sep_token='[SEP]')\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        self.dp1 = torch.nn.Dropout(p=0.1, inplace=False)\n",
    "        self.fc1 = torch.nn.Linear(2304, num_labels, bias=True)\n",
    "\n",
    "    def forward(self,sentence,label,device):\n",
    "        prepare_s  = [split_v2(itm, \" [SEP] \", 1) for itm in sentence]\n",
    "        tokens     = [self.tokenizer(itm,padding='max_length',truncation=True,max_length=self.len_embeddings,return_tensors=\"pt\").to(device) for itm in prepare_s]\n",
    "        embeddings = [self.model(**itm, output_hidden_states=True, return_dict=True).pooler_output for itm in tokens]\n",
    "        stack      = [torch.cat([itm, torch.abs(itm[0] - itm[1]).unsqueeze(0)]) for itm in embeddings]\n",
    "\n",
    "        x = torch.stack(stack)\n",
    "        pred = self.fc1(self.dp1(x.flatten(1)))\n",
    "        loss = self.loss_fn(pred, label)\n",
    "        return torch.unsqueeze(loss, -1), torch.argmax(pred, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CVdDP6zTKHC"
   },
   "source": [
    "### 2xSiameseBert (weight Sharing) -> CosSim + CosEmbLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9GdL5pfOC-o"
   },
   "outputs": [],
   "source": [
    "class ClassificationModel_V5(torch.nn.Module):\n",
    "    \"\"\"Siamese Networks Structure combined with Cosine Simmilarity of the two output embeddings with CosineEmbeddingLoss\n",
    "    \n",
    "    #https://www.sbert.net/examples/training/nli/README.html\n",
    "    #https://arxiv.org/pdf/1908.10084.pdf\n",
    "    \n",
    "    model.forward():\n",
    "    | Split sentence in 2 pieces with the [SEP] token as  Splitter\n",
    "    | 2xTokenize Sentence\n",
    "    | 2x Prediction\n",
    "    | Stack Items\n",
    "    | Calculate Loss + class_to_cos mapping\n",
    "    | Return Loss and Prediction + cos_to_class mapping\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,model_name:str=\"distilbert-base-uncased\", num_labels:int=2,len_embeddings:int=512):\n",
    "        super(ClassificationModel_V5,self).__init__()\n",
    "        self.len_embeddings = len_embeddings\n",
    "        self.model = transformers.AutoModel.from_pretrained(model_name,max_position_embeddings=self.len_embeddings,ignore_mismatched_sizes=True)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name,do_lower_case=True,sep_token='[SEP]')\n",
    "        self.loss_fn = torch.nn.CosineEmbeddingLoss()\n",
    "        self.cos_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self,sentence,label, device):\n",
    "        prepare_s  = [split_v2(itm, \" [SEP] \", 1) for itm in sentence]\n",
    "        tokens     = [self.tokenizer(itm,padding='max_length',truncation=True,max_length=self.len_embeddings,return_tensors=\"pt\").to(device) for itm in prepare_s]\n",
    "        embeddings = [self.model(**itm, output_hidden_states=True, return_dict=True).pooler_output for itm in tokens]\n",
    "        out      = torch.stack(embeddings).swapaxes(0,1)\n",
    "\n",
    "        loss = self.loss_fn(out[0], out[1], class_to_cos(label))\n",
    "        cos = self.cos_sim(out[0], out[1])\n",
    "        return torch.unsqueeze(loss, -1), cos_to_class(cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FLzFMb_TQtN"
   },
   "source": [
    "### 2xSiameseBert (weight Sharing) -> CosSim + MSE or L1 or SoftMargin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92xNgmiM6xVD"
   },
   "outputs": [],
   "source": [
    "class ClassificationModel_V6(torch.nn.Module):\n",
    "    \"\"\"Siamese Networks Structure combined with Cosine Simmilarity of the two output embeddings with other Loss functions\n",
    "    \n",
    "    #https://www.sbert.net/examples/training/nli/README.html\n",
    "    #https://arxiv.org/pdf/1908.10084.pdf\n",
    "    \n",
    "    model.forward():\n",
    "    | Split sentence in 2 pieces with the [SEP] token as  Splitter\n",
    "    | 2xTokenize Sentence\n",
    "    | 2x Prediction\n",
    "    | Stack Items\n",
    "    | Calculate Loss + class_to_cos mapping\n",
    "    | Return Loss and Prediction + cos_to_class mapping\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,model_name:str=\"distilbert-base-uncased\", num_labels:int=2,len_embeddings:int=512, v:int=1):\n",
    "        super(ClassificationModel_V6,self).__init__()\n",
    "        self.len_embeddings = len_embeddings\n",
    "        self.model = transformers.AutoModel.from_pretrained(model_name,max_position_embeddings=self.len_embeddings,ignore_mismatched_sizes=True)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name,do_lower_case=True,sep_token='[SEP]')\n",
    "\n",
    "        if v == 1: self.loss_fn = torch.nn.MSELoss()\n",
    "        if v == 2: self.loss_fn = torch.nn.L1Loss()\n",
    "        if v == 3: self.loss_fn = torch.nn.SoftMarginLoss()\n",
    "        self.cos_sim = torch.nn.CosineSimilarity(dim=-1, eps=1e-9)\n",
    "\n",
    "    def forward(self,sentence,label, device):\n",
    "        prepare_s  = [split_v2(itm, \" [SEP] \", 1) for itm in sentence]\n",
    "        tokens     = [self.tokenizer(itm,padding='max_length',truncation=True,max_length=self.len_embeddings,return_tensors=\"pt\").to(device) for itm in prepare_s]\n",
    "        embeddings = [self.model(**itm, output_hidden_states=True, return_dict=True).pooler_output for itm in tokens]\n",
    "        out        = torch.stack(embeddings).swapaxes(0,1)\n",
    "\n",
    "        cos        = self.cos_sim(out[0], out[1])\n",
    "        loss       = self.loss_fn(cos, class_to_cos(label).float())\n",
    "        return torch.unsqueeze(loss, -1), cos_to_class(cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ-j0IHMTefe"
   },
   "source": [
    "### 2xSiameseBert (weight Sharing) -> (u, v)Linear + CELoss + SupConLoss (abandoned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzLvdWG8PiuY"
   },
   "outputs": [],
   "source": [
    "#https://github.com/princeton-nlp/SimCSE/blob/main/simcse/models.py\n",
    "#https://bhuvana-kundumani.medium.com/implementation-of-simcse-for-unsupervised-approach-in-pytorch-a3f8da756839\n",
    "#https://arxiv.org/pdf/2104.08821.pdf\n",
    "#https://arxiv.org/pdf/2109.04321.pdf\n",
    "#https://aclanthology.org/2023.semeval-1.91.pdf\n",
    "\n",
    "class SupConLoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cos_sim = torch.nn.CosineSimilarity(dim=-1, eps=1e-9)\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, features):\n",
    "      cos = self.cos_sim(features[0].unsqueeze(1), features[1].unsqueeze(0)) / self.temperature\n",
    "      labels = torch.arange(cos.size(0)).long()\n",
    "      return self.loss_fct(cos, labels)\n",
    "\n",
    "class ClassificationModel_V8(torch.nn.Module):\n",
    "    \"\"\"Siamese Networks Structure combined with Cosine Simmilarity and Contrastive Loss\n",
    "    \n",
    "    #https://github.com/princeton-nlp/SimCSE/blob/main/simcse/models.py\n",
    "    #https://bhuvana-kundumani.medium.com/implementation-of-simcse-for-unsupervised-approach-in-pytorch-a3f8da756839\n",
    "    #https://arxiv.org/pdf/2104.08821.pdf\n",
    "    #https://arxiv.org/pdf/2109.04321.pdf\n",
    "    #https://aclanthology.org/2023.semeval-1.91.pdf\n",
    "    \n",
    "    model.forward():\n",
    "    | Split sentence in 2 pieces with the [SEP] token as  Splitter\n",
    "    | 2xTokenize Sentence\n",
    "    | 2x Prediction\n",
    "    | Stack Items\n",
    "    | Classifier\n",
    "    | Calculate Loss\n",
    "    | Return Loss and Prediction\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,model_name:str=\"distilbert-base-uncased\", num_labels:int=2,len_embeddings:int=512, v:int=1):\n",
    "        super(ClassificationModel_V8,self).__init__()\n",
    "        self.len_embeddings = len_embeddings\n",
    "        self.model = transformers.AutoModel.from_pretrained(model_name,max_position_embeddings=self.len_embeddings,ignore_mismatched_sizes=True)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name,do_lower_case=True,sep_token='[SEP]')\n",
    "\n",
    "        #Feed Forward Network\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.classifier = torch.nn.Linear(1536, 2)\n",
    "        self.loss_ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        if v == 1: self.loss_scl = SupConLoss(temperature=0.5)\n",
    "        if v == 2: self.loss_scl = SupConLoss(temperature=0.25)\n",
    "        if v == 3: self.loss_scl = SupConLoss(temperature=0.125)\n",
    "        if v == 4: self.loss_scl = SupConLoss(temperature=0.05)\n",
    "\n",
    "    def forward(self,sentence,label, device):\n",
    "        prepare_s  = [split_v2(itm, \" [SEP] \", 1) for itm in sentence]\n",
    "        tokens     = [self.tokenizer(itm,padding=True,truncation=True,return_tensors=\"pt\").to(device) for itm in prepare_s]\n",
    "        embeddings = [self.model(**itm, output_hidden_states=True, return_dict=True).pooler_output for itm in tokens]\n",
    "\n",
    "        emb = torch.stack(embeddings)\n",
    "        logits = self.classifier(self.dropout(emb.flatten(1)))\n",
    "\n",
    "        loss = self.loss_ce(logits, label) + self.alpha * self.loss_scl(emb)\n",
    "        #loss = (1 - self.alpha) * self.loss_ce(logits, label) + self.alpha * self.loss_scl(emb)\n",
    "        return  torch.unsqueeze(loss, -1), torch.argmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MILAnRTlOAT"
   },
   "source": [
    "### Adapter BertModelForSequenceClassification + CELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GjQ-fR6UBQY"
   },
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertIntermediate, BertOutput, BertLayer, BertEncoder, BertAttention, BertSelfOutput\n",
    "from transformers import BertConfig, BertModel, BertForSequenceClassification\n",
    "\n",
    "class BottleneckAdapterBertConfig(BertConfig):\n",
    "    def __init__(self,adapter_residual:bool=True,\n",
    "                      add_attention_adapter:bool=True,\n",
    "                      add_intermediate_adapter:bool=True,\n",
    "                      add_output_adapter:bool=True,\n",
    "                      layers_to_adapt:list=list(range(12)),\n",
    "                      adapter_non_linearity:str=\"gelu\",\n",
    "                      adapter_latent_size:int=512,\n",
    "                      last_layer_dropout:float=0.2,\n",
    "                      hidden_size:float=0.2,\n",
    "                      **kwargs):\n",
    "        \"\"\"Config Definition of the Adapter Model\n",
    "\n",
    "        Args:\n",
    "            adapter_residual (bool, optional): Adds the Residual Adder term in the Bottelneck FFN. Defaults to True.\n",
    "            add_attention_adapter (bool, optional): Adds the Attention Adapter. Defaults to True.\n",
    "            add_intermediate_adapter (bool, optional): Adds the Intermediate Adapter. Defaults to True.\n",
    "            add_output_adapter (bool, optional): Adds the Output Adapter. Defaults to True.\n",
    "            layers_to_adapt (list, optional): How much layers needs to be changed. Defaults to list(range(12)).\n",
    "            adapter_non_linearity (str, optional): Which non-linearity function is used. Defaults to \"gelu\".\n",
    "            adapter_latent_size (int, optional): Size of the downsizing and upsizing Bottleneck. Defaults to 512.\n",
    "            last_layer_dropout (float, optional): Dropout Percentage. Defaults to 0.2.\n",
    "            hidden_size (float, optional): Size of the Tensor. Defaults to 0.2.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.adapter_residual = adapter_residual\n",
    "        self.add_attention_adapter = add_attention_adapter\n",
    "        self.add_intermediate_adapter = add_intermediate_adapter\n",
    "        self.add_output_adapter = add_output_adapter\n",
    "        self.last_layer_dropout = last_layer_dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers_to_adapt = layers_to_adapt\n",
    "        self.adapter_latent_size = adapter_latent_size\n",
    "        self.adapter_non_linearity = adapter_non_linearity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BottleneckAdapterLayer(torch.nn.Module):\n",
    "    \"\"\"Adapter Bottleneck Layer with a Downsizing, Activation and Upsizing FFN construct\n",
    "    \"\"\"\n",
    "    def __init__(self, config, feature_size=None):\n",
    "        \"\"\" Initialize the Adapter and Creating the Linear Layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.adapter_input_size = feature_size if feature_size else config.hidden_size\n",
    "        self.adapter_latent_size = config.adapter_latent_size\n",
    "        self.residual = config.adapter_residual\n",
    "\n",
    "        self.down_proj = torch.nn.Linear(self.adapter_input_size, self.adapter_latent_size) # down projection\n",
    "        self.non_linearity = get_nonlin_func(config.adapter_non_linearity) # non linearity\n",
    "        self.up_proj = torch.nn.Linear(self.adapter_latent_size, self.adapter_input_size) # up projection\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize the weights -> so that initially the whole Adapter layer is a near-identity function\n",
    "        \"\"\"\n",
    "        self.down_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        self.down_proj.bias.data.zero_()\n",
    "        self.up_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        self.up_proj.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Definition of the Adapter layer as described in many Papers\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input Tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output Tensor\n",
    "        \"\"\"\n",
    "        output = self.up_proj(self.non_linearity(self.down_proj(x)))\n",
    "        if self.residual: output = x + output\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class AdapterBertOutput(BertOutput):\n",
    "    \"\"\"Overrides BertOutput with the Adapter Version\n",
    "    and adding the BottleneckAdapterLayer as it is described in papers\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config, layer_index):\n",
    "        \"\"\"Initializing the new BottleneckAdapterLayer\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        self.add_adapter = layer_index in config.layers_to_adapt  and config.add_output_adapter\n",
    "        if self.add_adapter: self.output_adapter = BottleneckAdapterLayer(config)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        \"\"\"Reconnect the Paths to the definition\n",
    "        \"\"\"\n",
    "        hidden_states = self.dropout(self.dense(hidden_states))\n",
    "        if self.add_adapter: hidden_states = self.output_adapter(hidden_states) # adapter extension\n",
    "        return self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "class AdapterBertIntermediate(BertIntermediate):\n",
    "    \"\"\"Overrides BertIntermediate with the Adapter Version\n",
    "    and adding the BottleneckAdapterLayer as it is described in papers\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config, layer_index):\n",
    "        \"\"\"Initializing the new BottleneckAdapterLayer\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        self.add_adapter = layer_index in config.layers_to_adapt and config.add_intermediate_adapter\n",
    "        if self.add_adapter: self.intermediate_adapter = BottleneckAdapterLayer(config, feature_size=self.dense.out_features)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"Reconnect the Paths to the definition\n",
    "        \"\"\"\n",
    "        if self.add_adapter: hidden_states = self.intermediate_adapter(hidden_states) # adapter extension\n",
    "        return self.intermediate_act_fn(self.dense(hidden_states))\n",
    "\n",
    "class AdapterBertSelfOutput(BertSelfOutput):\n",
    "    \"\"\"Overrides BertSelfOutput with the Adapter Version\n",
    "    and adding the BottleneckAdapterLayer as it is described in papers\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config, layer_index):\n",
    "        \"\"\"Initializing the new BottleneckAdapterLayer\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        self.add_adapter = layer_index in config.layers_to_adapt  and config.add_attention_adapter\n",
    "        if self.add_adapter: self.attention_adapter = BottleneckAdapterLayer(config)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        \"\"\"Reconnect the Paths to the definition\n",
    "        \"\"\"\n",
    "        hidden_states = self.dropout(self.dense(hidden_states))\n",
    "        if self.add_adapter: hidden_states = self.attention_adapter(hidden_states) # adapter extension\n",
    "        return self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AdapterBertAttention(BertAttention):\n",
    "    \"\"\"Overrides BertAttention with the Adapter Version\n",
    "    \"\"\"\n",
    "    def __init__(self, config, layer_index):\n",
    "        super().__init__(config)\n",
    "        self.output = AdapterBertSelfOutput(config, layer_index)\n",
    "\n",
    "class AdapterBertLayer(BertLayer):\n",
    "    \"\"\"Overrides BertLayer with the Adapter Version\n",
    "    \"\"\"\n",
    "    def __init__(self, config, layer_index):\n",
    "        super().__init__(config)\n",
    "        self.attention = AdapterBertAttention(config, layer_index)\n",
    "        self.intermediate = AdapterBertIntermediate(config, layer_index)\n",
    "        self.output = AdapterBertOutput(config, layer_index)\n",
    "\n",
    "class AdapterBertEncoder(BertEncoder):\n",
    "    \"\"\"Overrides BertEncoder with the Adapter Version\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.layer = torch.nn.ModuleList([AdapterBertLayer(config, i) for i in range(config.num_hidden_layers)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AdapterBertModel(BertModel):\n",
    "    \"\"\"Overrides BertModel with the Adapter Version\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.encoder = AdapterBertEncoder(config)       \n",
    "        self.freeze_unfreeze_params_all(False)\n",
    "        self.freeze_unfreeze_params_adapter(config, True)\n",
    "\n",
    "    def freeze_unfreeze_params_adapter(self, config, requires_grad:bool):\n",
    "        \"\"\"Freeze and Unfreeze Adapter Parameters\n",
    "\n",
    "        Args:\n",
    "            config: Bert Config\n",
    "            requires_grad (bool): enable disable Gradient\n",
    "        \"\"\"\n",
    "        for i in range(config.num_hidden_layers):\n",
    "            if i in config.layers_to_adapt:\n",
    "                if config.add_attention_adapter:    self.freeze_unfreeze_params(self.encoder.layer[i].attention.output.attention_adapter.parameters(), requires_grad)\n",
    "                if config.add_intermediate_adapter: self.freeze_unfreeze_params(self.encoder.layer[i].intermediate.intermediate_adapter.parameters(), requires_grad)\n",
    "                if config.add_output_adapter:       self.freeze_unfreeze_params(self.encoder.layer[i].output.output_adapter.parameters(), requires_grad)\n",
    "    \n",
    "    def freeze_unfreeze_params(self, itm, requires_grad:bool):\n",
    "        \"\"\"Freeze and Unfreeze Parameters\n",
    "\n",
    "        Args:\n",
    "            itm: items to enable and disable gradient\n",
    "            requires_grad (bool): enable disable Gradient\n",
    "        \"\"\"\n",
    "        for param in itm: param.requires_grad = requires_grad\n",
    "\n",
    "    def freeze_unfreeze_params_all(self, requires_grad:bool):\n",
    "        \"\"\"Freeze and Unfreeze all Parameters\n",
    "\n",
    "        Args:\n",
    "            requires_grad (bool): enable disable Gradient\n",
    "        \"\"\"\n",
    "        for param in self.parameters(): param.requires_grad = requires_grad\n",
    "\n",
    "class AdapterBertForSequenceClassification(BertForSequenceClassification):\n",
    "    \"\"\"Inherited Class from BertForSequenceClassification overriding config and bert model\n",
    "    \"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.bert = AdapterBertModel(config)\n",
    "        #self.bert.freeze_unfreeze_params_all(True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationModel_V9(torch.nn.Module):\n",
    "    \"\"\"Adapter Model of AutoModelForSequenceClassification with CrossEntropyLoss as loss function\n",
    "    Override Classes\n",
    "    \n",
    "    model.forward():\n",
    "    | tokenize Sentence\n",
    "    | Prediction\n",
    "    | Calculate Loss\n",
    "    | Return Loss and Prediction\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,model_name:str=\"distilbert-base-uncased\", num_labels:int=2,len_embeddings:int=512):\n",
    "        super(ClassificationModel_V9,self).__init__()\n",
    "        self.len_embeddings = len_embeddings\n",
    "        config = BottleneckAdapterBertConfig.from_pretrained(model_name,\n",
    "                                                             adapter_non_linearity=\"gelu\",\n",
    "                                                             add_attention_adapter=True,\n",
    "                                                             add_intermediate_adapter=False,\n",
    "                                                             add_output_adapter=True)\n",
    "        self.model = AdapterBertForSequenceClassification.from_pretrained(model_name,num_labels=num_labels,max_position_embeddings=self.len_embeddings,ignore_mismatched_sizes=True,config=config)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name,do_lower_case=True,sep_token='[SEP]')\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self,sentence,label,device):\n",
    "        token = self.tokenizer(sentence,add_special_tokens=True,padding='max_length',truncation=True,max_length=self.len_embeddings,return_tensors=\"pt\").to(device)\n",
    "        pred = self.model(**token,labels=label)\n",
    "        loss = self.loss_fn(pred.logits, label)\n",
    "        return torch.unsqueeze(loss, -1), torch.argmax(pred.logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZJuA7uqSgky"
   },
   "source": [
    "### Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fi8fJR8od_B-"
   },
   "outputs": [],
   "source": [
    "class ClassificationModule(pl.LightningModule):\n",
    "    def __init__(self, lr=0.001, model_name=\"bert-base-uncased\", num_labels=2, len_embeddings=512, seed_val=42, model_version=0, num_warmup_steps=20, **kwargs):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            lr (float, optional): Learning Rate for the Optimizer. Defaults to 0.001.\n",
    "            model_name (str, optional): Name of Huggingface Model. Defaults to \"bert-base-uncased\".\n",
    "            num_labels (int, optional): Number of Classes for Classification. Defaults to 2.\n",
    "            len_embeddings (int, optional): Lenght of the Embedding for tokenizer. Defaults to 512.\n",
    "            seed_val (int, optional): Seed value. Defaults to 42.\n",
    "            model_version (int, optional): Versions of the defined Classification Models. Defaults to 0.\n",
    "            num_warmup_steps (int, optional): Warmup Steps (deacitvated). Defaults to 20.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        print(self.hparams)\n",
    "\n",
    "        #Setting the Seed\n",
    "        #random.seed(self.hparams.seed_val) <---- DO NOT TOUCH else PERMUTATION on DATASET does not work\n",
    "        np.random.seed(self.hparams.seed_val)\n",
    "        torch.manual_seed(self.hparams.seed_val)\n",
    "        torch.cuda.manual_seed_all(self.hparams.seed_val)\n",
    "\n",
    "        #Select Model\n",
    "        assert model_version, \"No model Version Selected!\"\n",
    "        if  model_version == 1:   self.model = ClassificationModel_V1(model_name=self.hparams.model_name, num_labels=self.hparams.num_labels, len_embeddings=self.hparams.len_embeddings)\n",
    "        if  model_version == 2:   self.model = ClassificationModel_V2(model_name=self.hparams.model_name, num_labels=self.hparams.num_labels, len_embeddings=self.hparams.len_embeddings)\n",
    "        if  model_version == 3:   self.model = ClassificationModel_V3(model_name=self.hparams.model_name, num_labels=self.hparams.num_labels, len_embeddings=self.hparams.len_embeddings)\n",
    "        if  model_version == 4:   self.model = ClassificationModel_V4(model_name=self.hparams.model_name, num_labels=self.hparams.num_labels, len_embeddings=self.hparams.len_embeddings)\n",
    "        if  model_version == 5:   self.model = ClassificationModel_V5(model_name=self.hparams.model_name, num_labels=self.hparams.num_labels, len_embeddings=self.hparams.len_embeddings)\n",
    "        if  model_version == 6:   self.model = ClassificationModel_V6(model_name=self.hparams.model_name, num_labels=self.hparams.num_labels, len_embeddings=self.hparams.len_embeddings, v=1)\n",
    "        if  model_version == 7:   self.model = ClassificationModel_V6(model_name=self.hparams.model_name, num_labels=self.hparams.num_labels, len_embeddings=self.hparams.len_embeddings, v=2)\n",
    "        if  model_version == 8:   self.model = ClassificationModel_V6(model_name=self.hparams.model_name, num_labels=self.hparams.num_labels, len_embeddings=self.hparams.len_embeddings, v=3)\n",
    "        if  model_version == 9:   self.model = ClassificationModel_V8(model_name=self.hparams.model_name, num_labels=self.hparams.num_labels, len_embeddings=self.hparams.len_embeddings, v=1)\n",
    "        if  model_version == 10:  self.model = ClassificationModel_V8(model_name=self.hparams.model_name, num_labels=self.hparams.num_labels, len_embeddings=self.hparams.len_embeddings, v=2)\n",
    "        if  model_version == 11:  self.model = ClassificationModel_V8(model_name=self.hparams.model_name, num_labels=self.hparams.num_labels, len_embeddings=self.hparams.len_embeddings, v=3)\n",
    "        if  model_version == 12:  self.model = ClassificationModel_V8(model_name=self.hparams.model_name, num_labels=self.hparams.num_labels, len_embeddings=self.hparams.len_embeddings, v=4)\n",
    "        if  model_version == 13:  self.model = ClassificationModel_V9(model_name=self.hparams.model_name, num_labels=self.hparams.num_labels, len_embeddings=self.hparams.len_embeddings)\n",
    "        \n",
    "        #Initialize Save Registers for values\n",
    "        self.train_step_outputs = {\"loss\": [], \"pred\": [], \"target\": []}\n",
    "        self.val_step_outputs   = {\"loss\": [], \"pred\": [], \"target\": []}\n",
    "        self.test_step_outputs  = {\"loss\": [], \"pred\": [], \"target\": []}\n",
    "\n",
    "    def shared_step(self, step_outputs:dict, batch, stage:str) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            step_outputs (dict): Save Registry\n",
    "            batch (_type_): Current batch which needs to be Processed\n",
    "            stage (str): train, test, validation\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Loss of the batch\n",
    "        \"\"\"\n",
    "        \n",
    "        #Run the batch trough the model on the stage=oneof(train, vaild, test) and saving the results in the desired Registers \n",
    "        batch = batch if \"image\" in batch.keys() else list(map(batch.get,filter(lambda k: k<=self.current_epoch,batch.keys())))[-1] # Training with several DS depending on epoch or normal\n",
    "        loss, pred = self.model(sentence=batch[\"image\"], label=batch[\"label\"], device=self.device) #Run the Prediction from a batch\n",
    "\n",
    "        #Save Results\n",
    "        step_outputs[\"loss\"].append(loss)\n",
    "        step_outputs[\"pred\"].append(pred)\n",
    "        step_outputs[\"target\"].append(batch[\"label\"])\n",
    "        return loss\n",
    "\n",
    "    def shared_epoch_end(self, step_outputs:dict, stage:str):\n",
    "        \"\"\"Metric Calculations\n",
    "\n",
    "        Args:\n",
    "            step_outputs (dict): Registry of Calculated Entries of each Batch\n",
    "            stage (str): train, test, validation Stage\n",
    "        \"\"\"\n",
    "        #Concatenating all batch results and clear Registers\n",
    "        pred   = torch.cat(step_outputs[\"pred\"]).cpu()\n",
    "        step_outputs[\"pred\"].clear()\n",
    "        target = torch.cat(step_outputs[\"target\"]).cpu()\n",
    "        step_outputs[\"target\"].clear()\n",
    "        loss = torch.cat(step_outputs[\"loss\"]).cpu()\n",
    "        step_outputs[\"loss\"].clear()\n",
    "\n",
    "\n",
    "        #Metric Calculations\n",
    "        metrics = {stage+'_loss':      loss.mean().item(),\n",
    "                   stage+'_accurancy': accuracy_score(target, pred),\n",
    "                   stage+'_precision': precision_score(target, pred),\n",
    "                   stage+'_recall':    recall_score(target, pred),\n",
    "                   stage+'_f1':        f1_score(target, pred)}\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "        \n",
    "        #Confusion Matrix Calculations\n",
    "        #https://torchmetrics.readthedocs.io/en/stable/classification/confusion_matrix.html\n",
    "        #https://matplotlib.org/stable/users/explain/colors/colormaps.html\n",
    "        confusion = torchmetrics.ConfusionMatrix(num_classes=self.hparams.num_labels, task=\"multiclass\")\n",
    "        confusion(pred, target)\n",
    "        fig_, ax_ = plot_confusion_matrix(conf_mat=confusion.compute().detach().cpu().numpy().astype(int),show_absolute=True,show_normed=True,colorbar=True,cmap='Blues')\n",
    "        self.logger.experiment.add_figure(stage+\"_confusion_matrix\", fig_, self.current_epoch)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(self.train_step_outputs, batch, \"train\")\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        return self.shared_epoch_end(self.train_step_outputs, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(self.val_step_outputs, batch, \"valid\")\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        return self.shared_epoch_end(self.val_step_outputs, \"valid\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.shared_step(self.test_step_outputs, batch, \"test\")\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        return self.shared_epoch_end(self.test_step_outputs, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configurates the Optimizer and Scheduler\n",
    "        #https://huggingface.co/docs/transformers/en/main_classes/optimizer_schedules\n",
    "\n",
    "        Returns:\n",
    "            function: Choosen Optimizer\n",
    "        \"\"\"\n",
    "        #optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        #scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.hparams.num_warmup_steps, num_training_steps=self.trainer.max_epochs)\n",
    "        #return [optimizer], [scheduler]\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBxeivRXfoNN"
   },
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5m6121dWfYNh"
   },
   "source": [
    "## Start Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qf-zyRDofXGS"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "%tensorboard --logdir /content/drive/MyDrive/lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odPGYr7Fe2FM"
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dC_ew9c-WTG2"
   },
   "outputs": [],
   "source": [
    "#data = DataModule(batch_size=8, num_workers=0, permutate=False, strategy=\"\")\n",
    "data = DataModule(batch_size=16, num_workers=os.cpu_count(), permutate=False, strategy=\"\")\n",
    "\n",
    "#data = DataModule(batch_size=2, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedDataset\", ds_names=[\"snli\"])\n",
    "#data = DataModule(batch_size=2, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedLoader\", ds_names=[\"healthver\"])\n",
    "#data = DataModule(batch_size=2, num_workers=os.cpu_count(), permutate=False, strategy=\"Pre-Post:50\", ds_names=[\"scifact\"])\n",
    "\n",
    "#data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedDataset\", ds_names=[\"snli\", \"scifact\", \"healthver\"])\n",
    "#data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedLoader\", ds_names=[\"snli\", \"scifact\", \"healthver\"])\n",
    "#data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"Pre-Post:25:50:100\", ds_names=[\"snli\", \"scifact\", \"healthver\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ClassificationModule(lr=5.000e-5, model_name=\"princeton-nlp/sup-simcse-bert-base-uncased\", len_embeddings=512, seed_val=0, model_version=114)\n",
    "model = ClassificationModule(lr=5.000e-6, model_name=\"bert-base-uncased\", len_embeddings=512, seed_val=0, model_version=6)\n",
    "logger = pl.loggers.TensorBoardLogger(save_dir=LOG_FOLDER, version=f\"bert-v6-512_5.000e-6_seed-0\", name=\"lightning_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLd8EQGaWUYk"
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(accelerator=\"auto\",max_epochs=50,logger=logger, enable_checkpointing=False)\n",
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ineyb0bXWVvv"
   },
   "outputs": [],
   "source": [
    "#print(model)\n",
    "#trainer.save_checkpoint(\"example.ckpt\")\n",
    "#new_model = ClassificationModule.load_from_checkpoint(checkpoint_path=\"example.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40PlF1w8WDsA"
   },
   "source": [
    "# Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfraoOsOWDsC"
   },
   "source": [
    "## Aggregation Test\n",
    "\n",
    "Does all Combination of Strategy and Dataset works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSJDv_V_WDsD"
   },
   "outputs": [],
   "source": [
    "#data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"\")\n",
    "#data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedLoader\", ds_names=[\"healthver\"])\n",
    "#data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedDataset\", ds_names=[\"scifact\", \"healthver\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTwnYxGDWDsE"
   },
   "outputs": [],
   "source": [
    "#for batch in data.train_dataloader(): print(batch[\"label\"], batch[\"image\"])\n",
    "#for batch in data.val_dataloader(): print(batch[\"label\"], batch[\"image\"])\n",
    "#for batch in data.test_dataloader(): print(batch[\"label\"], batch[\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-5evr1VWDsF"
   },
   "outputs": [],
   "source": [
    "data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"\")\n",
    "len(data.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d1l-wCxWDsH"
   },
   "outputs": [],
   "source": [
    "data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedDataset\", ds_names=[\"healthver\"])\n",
    "len(data.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onSBrQbmLyE5"
   },
   "outputs": [],
   "source": [
    "data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedDataset\", ds_names=[\"scifact\"])\n",
    "len(data.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcG5ZPaE_Idh"
   },
   "outputs": [],
   "source": [
    "data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedDataset\", ds_names=[\"snli\"])\n",
    "len(data.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvS7UCL4WDsG"
   },
   "outputs": [],
   "source": [
    "data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedLoader\", ds_names=[\"healthver\"])\n",
    "len(data.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkB9uKU8LwtM"
   },
   "outputs": [],
   "source": [
    "data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedLoader\", ds_names=[\"scifact\"])\n",
    "len(data.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqz7dWF2_KK_"
   },
   "outputs": [],
   "source": [
    "data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedLoader\", ds_names=[\"snli\"])\n",
    "len(data.train_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4duuBxbnWDsH"
   },
   "source": [
    "## SemEval Dataset\n",
    "\n",
    "Distribution Analysis of SemEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "si-GU89nWDsI"
   },
   "outputs": [],
   "source": [
    "data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3Px5epsWDsJ"
   },
   "outputs": [],
   "source": [
    "label = np.array([i for batch in data.train_dataloader() for i in batch[\"label\"].numpy()])\n",
    "section = np.array([i.split(\"[SEP]\")[1] for batch in data.train_dataloader() for i in batch[\"image\"]])\n",
    "print(\"label distribution in Train Dataloader:\", Counter(label))\n",
    "print(\"section distribution in Train Dataloader:\", Counter(section))\n",
    "print(\"section distribution in Train Dataloader for Label 0:\", Counter(section[label == 0]))\n",
    "print(\"section distribution in Train Dataloader for Label 1:\", Counter(section[label == 1]))\n",
    "\n",
    "print()\n",
    "\n",
    "label = np.array([i for batch in data.val_dataloader() for i in batch[\"label\"].numpy()])\n",
    "section = np.array([i.split(\"[SEP]\")[1] for batch in data.val_dataloader() for i in batch[\"image\"]])\n",
    "print(\"label distribution in Valid Dataloader:\", Counter(label))\n",
    "print(\"section distribution in Val Dataloader:\", Counter(section))\n",
    "print(\"section distribution in Val Dataloader for Label 0:\", Counter(section[label == 0]))\n",
    "print(\"section distribution in Val Dataloader for Label 1:\", Counter(section[label == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ck4dXa1-d3t"
   },
   "source": [
    "## Histogram\n",
    "\n",
    "Generating a Histogram of the Distribution of the Sencente lenght or Tokenized length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9XZi7Gv-bLd"
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"\")\n",
    "#data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedDataset\", ds_names=[\"healthver\"])\n",
    "#data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedDataset\", ds_names=[\"scifact\"])\n",
    "#data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedDataset\", ds_names=[\"snli\"])\n",
    "#data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedDataset\", ds_names=[\"scifact\", \"healthver\"])\n",
    "#data = DataModule(batch_size=8, num_workers=os.cpu_count(), permutate=False, strategy=\"CombinedDataset\", ds_names=[\"scifact\", \"healthver\", \"snli\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcySXCB2WDsM"
   },
   "outputs": [],
   "source": [
    "train_len_str   = np.array([len(i) for batch in data.train_dataloader() for i in batch[\"image\"]])\n",
    "train_num_words = np.array([len(list(map(len, i.split()))) for batch in data.train_dataloader() for i in batch[\"image\"]])\n",
    "train_tok_words = np.array([len(tokenizer.tokenize(i,add_special_tokens=True)) for batch in data.train_dataloader() for i in batch[\"image\"]])\n",
    "\n",
    "print(len(train_len_str), len(train_num_words), len(train_tok_words))\n",
    "print(np.sum(train_tok_words <= 512), np.sum(train_tok_words > 512))\n",
    "\n",
    "print(\"Lengt of String in Train Dataset:\", f\"min={np.min(train_len_str).round(2)}\",\n",
    "                                           f\"max={np.max(train_len_str).round(2)}\",\n",
    "                                           f\"mean={np.mean(train_len_str).round(2)}\",\n",
    "                                           f\"median={np.median(train_len_str).round(2)}\")\n",
    "\n",
    "print(\"Number of Words in Train Dataset:\", f\"min={np.min(train_num_words).round(2)}\",\n",
    "                                           f\"max={np.max(train_num_words).round(2)}\",\n",
    "                                           f\"mean={np.mean(train_num_words).round(2)}\",\n",
    "                                           f\"median={np.median(train_num_words).round(2)}\")\n",
    "\n",
    "print(\"Tokenized Words in Train Dataset:\", f\"min={np.min(train_tok_words).round(2)}\",\n",
    "                                           f\"max={np.max(train_tok_words).round(2)}\",\n",
    "                                           f\"mean={np.mean(train_tok_words).round(2)}\",\n",
    "                                           f\"median={np.median(train_tok_words).round(2)}\")\n",
    "\n",
    "\n",
    "valid_len_str   = np.array([len(i) for batch in data.val_dataloader() for i in batch[\"image\"]])\n",
    "valid_num_words = np.array([len(list(map(len, i.split()))) for batch in data.val_dataloader() for i in batch[\"image\"]])\n",
    "valid_tok_words = np.array([len(tokenizer.tokenize(i,add_special_tokens=True)) for batch in data.val_dataloader() for i in batch[\"image\"]])\n",
    "\n",
    "print(len(valid_len_str), len(valid_num_words), len(valid_tok_words))\n",
    "print(np.sum(valid_tok_words <= 512), np.sum(valid_tok_words > 512))\n",
    "\n",
    "print(\"Lengt of String in Val Dataset:\", f\"min={np.min(valid_len_str).round(2)}\",\n",
    "                                         f\"max={np.max(valid_len_str).round(2)}\",\n",
    "                                         f\"mean={np.mean(valid_len_str).round(2)}\",\n",
    "                                         f\"median={np.median(valid_len_str).round(2)}\")\n",
    "\n",
    "print(\"Number of Words in Val Dataset:\", f\"min={np.min(valid_num_words).round(2)}\",\n",
    "                                         f\"max={np.max(valid_num_words).round(2)}\",\n",
    "                                         f\"mean={np.mean(valid_num_words).round(2)}\",\n",
    "                                         f\"median={np.median(valid_num_words).round(2)}\")\n",
    "\n",
    "print(\"Tokenized Words in Val Dataset:\", f\"min={np.min(valid_tok_words).round(2)}\",\n",
    "                                         f\"max={np.max(valid_tok_words).round(2)}\",\n",
    "                                         f\"mean={np.mean(valid_tok_words).round(2)}\",\n",
    "                                         f\"median={np.median(valid_tok_words).round(2)}\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5), sharex=True)\n",
    "num_bins = 75\n",
    "bins=np.histogram(np.hstack((train_num_words,train_tok_words, valid_num_words,valid_tok_words)), bins=num_bins)[1] #get the bin edges\n",
    "\n",
    "\n",
    "axs[0].xaxis.set_major_locator(ticker.MaxNLocator(15))\n",
    "axs[0].hist(train_num_words, bins=bins, edgecolor='black', alpha=0.75, label=f'Words (mean={int(np.mean(train_num_words))})')\n",
    "axs[0].hist(train_tok_words, bins=bins, edgecolor='black', alpha=0.5, label=f'Token (mean={int(np.mean(train_tok_words))})')\n",
    "axs[0].set_title(f\"Training Histogram\")\n",
    "axs[0].set_xlabel(\"Occurancies in Sentence\")\n",
    "axs[0].set_ylabel(\"Occurancies in Dataset\")\n",
    "#axs[0].axvline(x=512, color='red', lw=1, ls='--', label='512 Token Mark')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].xaxis.set_major_locator(ticker.MaxNLocator(15))\n",
    "axs[1].hist(valid_num_words, bins=bins, edgecolor='black', alpha=0.75, label=f'Words (mean={int(np.mean(valid_num_words))})')\n",
    "axs[1].hist(valid_tok_words, bins=bins, edgecolor='black', alpha=0.5, label=f'Token (mean={int(np.mean(valid_tok_words))})')\n",
    "axs[1].set_title(\"Valid Histogram\")\n",
    "axs[1].set_xlabel(\"Occurancies in Sentence\")\n",
    "axs[1].set_ylabel(\"Occurancies in Dataset\")\n",
    "#axs[1].axvline(x=512, color='red', lw=1, ls='--', label='512 Token Mark')\n",
    "axs[1].legend()\n",
    "\n",
    "name = LOG_FOLDER + \"/histogram.svg\"\n",
    "plt.savefig(name, format=\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgrCq6LAWDsP"
   },
   "source": [
    "## Token\n",
    "\n",
    "Analyzation hot the tokens are built for a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T81K7kq8WDsQ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i, batch in enumerate(data.val_dataloader()):\n",
    "  for j, itm in enumerate(batch[\"image\"]):\n",
    "    tok = tokenizer.tokenize(itm,add_special_tokens=True)\n",
    "    print(i, j, len(tok), tok)\n",
    "  print()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "_Gz0-ZOZhU5b",
    "3COzQBSiWoXo",
    "jEPDgKEBRC2z",
    "4590QaN8N0__",
    "YRYGJ3uzPQEt",
    "Hufo5YTOdhNB",
    "nBXmlO8tdnpa",
    "kjAUAIyaSaNz",
    "y8O8avZuSR4F",
    "iG_Q2vYhStGT",
    "T7Fa6cGeS5t-",
    "W0CAoxwQS9bz",
    "3CVdDP6zTKHC",
    "qJ-j0IHMTefe",
    "PfoOTuYLlIGI",
    "4duuBxbnWDsH"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
